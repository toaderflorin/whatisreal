---
layout: post
title:  "How Games Figure Out What's Visible In A Level"
date:   2017-10-25 11:13:11 +0300
description: "In my previous game related article, I went a bit over the history of id software games spanning from Wolfenstein to Quake and I explained how the tech handles things such as occlusion culling (determining which parts of the level are visible from the player’s point of view), and how lighting works...
"
image: "/images/unreal.jpg"
---

In my previous game related article, I went a bit over the history of id software games spanning from Wolfenstein to Quake and I explained how the tech handles things such as occlusion culling (determining which parts of the level are visible from the player’s point of view), and how lighting works. Now, I plan on expanding a bit on this: namely I want to have a look at how modern engines are doing it.

As you might have noticed, technology has evolved a bit in the meantime. Have a look at the following scene from Bloodborne.

![image-title-here](/images/bloodborne.jpg){:class="img-responsive"}

When your level looks like this, the Quake 1 approach (binary space partitioning trees) is out of the question because there are literally million of polygons there. This is what developers typically call polygon soup — creating a level like this involves painstakingly modelling assets (the sculptures that you see, the tomb stones etc.) in a tool such as 3D Studio Max or Maya (or using photogrammetry) and then putting them together in whatever tool the company uses to create a coherent level design. This is how game designers are able to create such compelling worlds, but it comes as a cost: it’s hard for the engine to make sense of the world—it’s just a huge collection of polygons. One particular challenge is figuring out what is visible and what is occluded from any point in space.

![image-title-here](/images/portals.png){:class="img-responsive"}

An approach that was popular for a while was having level designers having to manually create areas and place portals in the game editor (Doom 3 did this), but there are several problems with this approach. First of all it’s tedious, because it involves a lot of manual work. If the designer wants to change the level he / she needs to also change the placement of the portals and so forth, and if there are alignment issues, it can cause bleeding and other unwanted artefacts. Second, portals works great for interior labyrinthine levels such as Alien Isolation (or Doom 3, for that matter). They don’t work well for vast outdoor areas and they are especially bad if your game mixes indoor and outdoor areas. And last but certainly not least, this approach only works for static geometry.

What you ideally want is a way that just works automatically with minimal preprocessing that is able to handle all these issues. This is where hardware occlusion queries enter the picture — as the name says, this is a feature which is supported natively by current video graphics cards.

![image-title-here](/images/unreal.jpg){:class="img-responsive"}

Unreal Engine 4 uses this approach as the default way of doing visible surface determination and I will try to explain it in layman’s terms.

When rendering to the screen, the graphics card maintains an image buffer (the colors of each pixel), and a depth buffer or Z buffer which looks like this picture (it’s just displays the distance from the camera to the nearest occluder).

![image-title-here](/images/depthmap.png){:class="img-responsive"}

You can render the polygons of a level in whatever order you want—the graphics card renders pixel by pixel and always checks if the current point’s Z coordinate is further of closer than the current value. If it’s closer, than the point is visible, if it’s further it means the pixel is hidden. So this is already a form of optimization because the renderer can skip the complex shading of the pixel (lighting, texturing, reflections etc.). However, there is still the overhead of doing the mathematical 3D transformations such as rotation, translation and projection on screen space for all the polygons in a certain model, and considering that objects contain tens or hundreds of thousands of polygons, this is quite a lot. Wouldn’t it be cool to be able to through out entire parts of the level? This is in fact what hardware occlusion culling does.

The simplest implementation of HOQ would look this:

1. You render one frame without occlusion culling which means you now have a depth buffer filled out.

2. On each subsequent frame, go through your level object by object, and you use a query to see if that object is visible, but instead of using the complex version of the object, you use something more simple like a bounding box. When the graphics card is in query mode, it’s not actually writing to the image buffer, it just checks to see if any of the pixels of the bounding box would pass the Z buffer test.

3. If there are any pixels that, pass the Z buffer test, then you render the complex object. The reason for using a bounding box is you always want to cull conservatively. It’s better to render objects that are not actually visible (they will by culled by the Z test anyway), than to actually miss the edges of some objects — this would lead to really ugly visual artefacts.

You might have noticed that we are relying the depth-buffer from the previous frame. This sometimes leads to flicker, and objects popping into existence (especially when the player turns fast), which has lead to searching for alternative ways of doing VSD. In Frostbite (the engine behind Battlefront and Battlefield 1), they are essentially using the same algorithm that’s used in HOQs, but they are doing it in software, so they can bypass hardware limitations and can do it in the same frame.

Full details [here](http://blog.selfshadow.com/publications/practical-visibility).

Lately, there is however a new player at the table. That player is Umbra. And it’s really popular: Doom, Witcher 3, Call of Duty: Ghosts, Destiny, Alan Wake, Mass Effect 2 and 3 are just a few of the games which are using it. At it’s core, Umbra uses portals and cells, but the difference is that the cells are always cubical. The tech simply takes the game world and subdivides it in cubes.

![image-title-here](/images/umbra.jpeg){:class="img-responsive"}

If a cube contains, it subdivides that cube in smaller cubes, and so forth. The end result is a voxelized representation of the world, but the voxels are actually cells and they are connected to adjacent cells via portals. To figure out what is visible, Umbra takes an unorthodox approach — it starts from the observer point of view and actually draws the portals first in a buffer (as opposed to actually drawing the occluders) using software rasterization. The end result is software buffer where we have a 1 indicates the observer can see through and a 0 indicating occlusion. The to figure out if an object is visible, the algorithm just checks if the rendition of an object’s bounding box overlaps any pixel that is see-through.

And there you have it. That’s how the magic works.
